{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cda9bc5-e322-4a85-81d0-85b7ba4a8537",
   "metadata": {},
   "source": [
    "# Exploring mixehr outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6684e8-abb3-4346-9793-9c3e9b37bb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c24bb2-36d3-46bc-bd87-b17178b2a42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_subj_id_to_df(df, id_list):\n",
    "    \"\"\"\n",
    "    assuming the id_list and df have a 1:1 correspondence, \n",
    "    add id_list as a column\n",
    "    \"\"\"\n",
    "    df.insert(0, 'subj_id', id_list)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def rename_topic_df(df):\n",
    "    \"\"\"\n",
    "    add topic{index} as column name\n",
    "    \"\"\"\n",
    "    topic_names = [f'topic{x}' for x in range(len(df.columns))]\n",
    "    df.columns = topic_names\n",
    "    return df\n",
    "\n",
    "def load_paths(topic_path, id_list_path):\n",
    "    \"\"\"\n",
    "    load csv of topics without hadm ids, and pickle of id list. Check their lengths are the same\n",
    "    Returns:\n",
    "        topic_df: df with each row as a patient, each column a topic, each value a likelihood of belonging to the topic, id_list: list of hadm_ids\n",
    "    \"\"\"\n",
    "    #pdb.set_trace()\n",
    "    topic_df = pd.read_csv(topic_path, header=None)\n",
    "    with open(id_list_path, 'rb') as id_list:\n",
    "        ids = pkl.load(id_list)\n",
    "    try:\n",
    "        id_len = len(ids)\n",
    "        topic_len = len(topic_df)\n",
    "        assert id_len == topic_len, (f'id length is {id_len} but topic df is length {topic_len}')\n",
    "    except:\n",
    "        #pdb.set_trace()\n",
    "        warnings.warn(f'id length is {id_len} but topic df is length {topic_len} on path {topic_path}. ignoring last patients for now')\n",
    "        if id_len > topic_len:\n",
    "            ids = ids[:topic_len]\n",
    "        if topic_len > id_len:\n",
    "            topic_df = topic_df.iloc[:id_len, :]\n",
    "    return topic_df, ids\n",
    "\n",
    "def add_matching_hadm(hadm_subj_csv, subj_df, hadm_id_path):\n",
    "    \"\"\"\n",
    "    Add hadm_ids to df according to hadm_id pickle\n",
    "    Args:\n",
    "        hadm_subj_csv: path to csv containing hadm_id and corresponding subj_id\n",
    "        subj_df: processed df which contains subj_id, which we want to append corresponding hadm_id to\n",
    "        hadm_id_path: path to pickle containing list of hadm_ids used \n",
    "    \"\"\"\n",
    "    with open(hadm_id_path, 'rb') as id_list:\n",
    "        hadm_ids = pkl.load(id_list)\n",
    "    \n",
    "    hadm_subj_df = pd.read_csv(hadm_subj_csv)\n",
    "    subj_ids = list(subj_df['subj_id'].values)\n",
    "    filtered_hadm_subj_df = hadm_subj_df[hadm_subj_df['HADM_ID'].isin(hadm_ids)]\n",
    "    sorted_hadm_ids = []\n",
    "    for subj_id in subj_ids:\n",
    "        # find the corresponding hadm_id\n",
    "        matching_rows = filtered_hadm_subj_df[filtered_hadm_subj_df['SUBJECT_ID'] == subj_id]\n",
    "        sorted_hadm_ids.append(matching_rows['HADM_ID'].values[0])\n",
    "        assert len(matching_rows) == 1, (f'greater than one match found for subj id {subj_id}')\n",
    "    \n",
    "    subj_df.insert(0, 'hadm_id', sorted_hadm_ids)\n",
    "    return subj_df\n",
    "\n",
    "def format_df_for_pipeline(topic_path, id_list_path, hadm_id_path):\n",
    "    \"\"\"\n",
    "    rename topic df with topics and hadm id\n",
    "    \"\"\"\n",
    "    df, id_list = load_paths(topic_path, id_list_path)\n",
    "    df = rename_topic_df(df)\n",
    "    df = add_subj_id_to_df(df, id_list)\n",
    "    df = add_matching_hadm('../data/mimic3/ADMISSIONS.csv', df, hadm_id_path) \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99346a-e6a3-4ae7-a231-e25f87a0621b",
   "metadata": {},
   "source": [
    "## define paths\n",
    "each key is a run type. \"labs\" is mixehr topics from labs results only. \"labs_notes\" is both labs and clinical notes, etc. \n",
    "K75 indicates 75 topics total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a40bc-ad7b-4532-a2b3-dca6c9fd3d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_topic_paths = {\n",
    "    'labs': 'test_mixehr_early_no_waveforms_train_mixehr_early_no_waveforms_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    'labs_notes': 'test_mixehr_early_no_waveforms_train_mixehr_early_no_waveforms_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    'labs_notes_ecg': 'test_mixehr_early_with_ecg_quantiles_train_mixehr_early_with_ecg_quantiles_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    'ecg': 'test_mixehr_early_only_ecg_quantiles_train_mixehr_early_only_ecg_quantiles_JCVB0_nmar_K75_iter156_metaphe'\n",
    "}\n",
    "\n",
    "val_topic_paths = val_topic_paths = {\n",
    " key: val.replace('test', 'vali') for key, val in test_topic_paths.items()   \n",
    "}\n",
    "\n",
    "train_topic_paths = {\n",
    "    'labs': 'train_mixehr_early_no_notes_no_waveforms_train_mixehr_early_no_notes_no_waveforms_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    'labs_notes': 'train_mixehr_early_no_waveforms_train_mixehr_early_no_waveforms_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    'labs_notes_ecg': 'train_mixehr_early_with_ecg_quantiles_train_mixehr_early_with_ecg_quantiles_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    #'ecg': 'train_mixehr_early_only_ecg_quantiles_train_mixehr_early_only_ecg_quantiles_JCVB0_nmar_K75_iter156_metaphe'\n",
    "}\n",
    "\n",
    "assert all([key_train == key_test for key_train, key_test in zip(test_topic_paths.keys(), train_topic_paths.keys())]), ('not all keys matched')\n",
    "modality_names = train_topic_paths.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315919c9-953f-44e9-b7b1-1ed56503ab99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_topic_paths = {\n",
    "    'labs': 'test_mixehr_no_waveforms_train_mixehr_no_waveforms_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    'labs_notes': 'test_mixehr_no_waveforms_train_mixehr_no_waveforms_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    'labs_notes_ecg': 'test_mixehr_with_ecg_quantiles_train_mixehr_with_ecg_quantiles_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    'ecg': 'test_mixehr_only_ecg_quantiles_train_mixehr_only_ecg_quantiles_JCVB0_nmar_K75_iter500_metaphe'\n",
    "}\n",
    "\n",
    "train_topic_paths = {\n",
    "    'labs': 'train_mixehr_no_notes_no_waveforms_train_mixehr_no_notes_no_waveforms_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    'labs_notes': 'train_mixehr_no_waveforms_train_mixehr_no_waveforms_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    'labs_notes_ecg': 'train_mixehr_with_ecg_quantiles_train_mixehr_with_ecg_quantiles_JCVB0_nmar_K75_iter500_metaphe',\n",
    "    'ecg': test_topic_paths['ecg'].replace('test', 'train')\n",
    "}\n",
    "\n",
    "val_topic_paths = {\n",
    "    key: val.replace('test', 'vali') for key, val in test_topic_paths.items()   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822171b9-41ee-43a3-9c1a-60614a9e68ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_paths(modality, split_name):\n",
    "    \"\"\"\n",
    "    return a dict containing the raw topic path, the save path for the processed df, \n",
    "    and the path to the id list\n",
    "    \"\"\"\n",
    "    save_base = '../data/supervised_pipeline'\n",
    "    data_base = '../data/mixehr/'\n",
    "    Path(save_base).mkdir(parents=True, exist_ok=True)\n",
    "    if split_name == 'train':\n",
    "        topic_path = train_topic_paths[modality] + '.csv'\n",
    "    elif split_name == 'test':\n",
    "        topic_path = test_topic_paths[modality] + '.csv'\n",
    "    elif split_name == 'vali':\n",
    "        topic_path = val_topic_paths[modality] + '.csv'\n",
    "    else:\n",
    "        raise ValueError(f'{split_name} must be either train or test')\n",
    "    paths = {\n",
    "        'save': os.path.join(save_base, f'{modality}_topics.csv'),\n",
    "        'raw_topics': os.path.join(data_base, topic_path),\n",
    "        'id_list': f'{data_base}/{split_name}_subj_ids_list.pkl',\n",
    "        'hadm_id_list': f'{data_base}/{split_name}_hadm_ids_list.pkl'\n",
    "    }\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc7926-af77-4ba2-8406-b1018a9a699c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "splits = ['train', 'vali', 'test']\n",
    "for modality in modality_names:\n",
    "    all_dfs = []\n",
    "    all_split_names = []\n",
    "    for split in splits:\n",
    "        paths = get_paths(modality, split)\n",
    "        df = format_df_for_pipeline(paths['raw_topics'], paths['id_list'], paths['hadm_id_list'])\n",
    "        split_name = [split] * len(df)\n",
    "        all_dfs.append(df)\n",
    "        all_split_names.extend(split_name)\n",
    "\n",
    "    all_dfs = pd.concat(all_dfs)\n",
    "    all_dfs.insert(0, 'split', all_split_names)\n",
    "    all_dfs.to_csv(paths['save'], index=False)\n",
    "    print(f'saved {modality} topics to {paths[\"save\"]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774bce59-f610-4a42-bab0-526a1ce1a971",
   "metadata": {
    "tags": []
   },
   "source": [
    "# add label information\n",
    "now we have a df for each set. next we want to combine it with LOS info \n",
    "this can be done in `make_matching_los_csv`, creating the file `labels_matched.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4547d46-b220-46cd-830a-6a1e7fce28e6",
   "metadata": {},
   "source": [
    "# filtering trajectory results\n",
    "big issue right now is we have many many ecg signals across time and we havent filtered. we should only use the first 12 hours, and we should only include the HADM_ids which have corresponding topics. Here, i will filter trajectories_with_features.csv for ts_idx=1, and hadm_id matching hadm_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879ebd6-5eca-44aa-9578-165ff6df882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_path = '../data/trajectories_with_features.csv'\n",
    "# now we filter for the first ts_index\n",
    "ecg_df = pd.read_csv(ecg_path)\n",
    "ecg_df = ecg_df[ecg_df['ts_idx'] == 1]\n",
    "ecg_savepath = '../data/filtered_ecg_features.csv'\n",
    "ecg_df.to_csv(ecg_savepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa970bf-456d-4b9c-94fd-52a041acf56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_matched_los_csv import main\n",
    "label_csv = 'mimic3/ADMISSIONS'\n",
    "label_cols = {\n",
    "    'los': 'length_of_stay',\n",
    "    'mort': 'HOSPITAL_EXPIRE_FLAG'\n",
    "}\n",
    "main(ecg_path, label_csv, label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65341aec-057c-4eda-9615-4b172d9d452c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31229e-6df5-48be-a3f8-17c57d96eff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
