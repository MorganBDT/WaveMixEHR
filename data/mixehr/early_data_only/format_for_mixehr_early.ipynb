{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c63e4-87f8-4432-8853-899fdffab98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from bidict import bidict\n",
    "import scipy\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad0713-4940-48d3-a3b5-20689fdde01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If True, prepare the training data including filtering for common values etc. \n",
    "PREPARE_TRAINING_DATA = True\n",
    "\n",
    "# If True, prepare the validation and testing datasets (using bidicts generated using training data)\n",
    "PREPARE_VALI_TEST_DATA = True\n",
    "\n",
    "# If True, make separate NOTEEVENTS.csv subset csvs for training, test, and vali\n",
    "MAKE_SEPARATE_NOTE_CSVS = True\n",
    "\n",
    "# If True, regenerate the dictionary of keywords for clinical notes (takes ~30-40 minutes)\n",
    "BUILD_NOTES_DICTIONARY = True\n",
    "\n",
    "# Combine the actual data into mixehr formatted text files\n",
    "COMBINE_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3480cb57-9b8d-48a6-b2e3-6f7b08aba2c0",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310cbe36-073c-41bb-a782-6dc31abde7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def only_these_subj_and_hadm_ids(df, subj_ids, hadm_ids):\n",
    "    df = df[df['SUBJECT_ID'].isin(subj_ids)]\n",
    "    df = df[df['HADM_ID'].isin(hadm_ids)]\n",
    "    return df\n",
    "              \n",
    "\n",
    "# Restrict to one hadm_id for each patient. \n",
    "def only_one_hadm_per_subj(df):\n",
    "    # Group by 'subject_id' and select one random 'hadm_id'\n",
    "    np.random.seed(0)\n",
    "    random_hadm_ids = df.groupby('SUBJECT_ID')['HADM_ID'].apply(lambda x: np.random.choice(x))\n",
    "\n",
    "    # Filter the dataframe to keep only the rows with the selected 'hadm_id' for each 'subject_id'\n",
    "    filtered_df = df[df['HADM_ID'].isin(random_hadm_ids)]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Keep only items within \"secs\" seconds of the start of the patient's ICU stay. \n",
    "def within_x_secs_of_admit(df, secs, icustays, col_name=\"CHARTTIME\"):\n",
    "    df = df[df[col_name].notna()]\n",
    "    \n",
    "    rgt = '%Y-%m-%d %H:%M:%S'\n",
    "    df[\"keep\"] = df.apply(lambda row: \\\n",
    "        (datetime.strptime(row[col_name], rgt) \\\n",
    "        - datetime.strptime(icustays[icustays[\"HADM_ID\"] == row[\"HADM_ID\"]].iloc[0][\"INTIME\"], rgt)).total_seconds() < secs, axis=1)\n",
    "    \n",
    "    df = df[df[\"keep\"]]\n",
    "    df = df.drop(labels=[\"keep\"], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Each df should have a patientId, typeId, pheId, and stateId column. \n",
    "# This function adds a \"freq\" column with a count, for each patientId and typeId, \n",
    "# of how many times a given pheId occurred along with a given stateId. \n",
    "# Essentially, duplicates are removed, and each unique row now has a \n",
    "# count of how many duplicates were in the dataframe before. \n",
    "def get_mixehr_format_with_freqs(df):\n",
    "    grpd = df.groupby([\"patientId\", \"typeId\", \"pheId\", \"stateId\"]).size().to_frame(\"freq\").reset_index()\n",
    "    return grpd[[\"patientId\", \"typeId\", \"pheId\", \"stateId\", \"freq\"]]\n",
    "\n",
    "\n",
    "def prepare_vali_test_sets(df, mapper, pheIdCol, idset):\n",
    "    df[\"pheId\"] = df[pheIdCol].map(mapper.inverse)\n",
    "    df = df.dropna(subset=[\"pheId\"])\n",
    "    df[\"pheId\"] = df[\"pheId\"].astype(int)\n",
    "    return df\n",
    "\n",
    "# Create a dataframe for the metadata file needed by mixehr. \n",
    "# Input mixehr_df should have columns \"patientId\", \"typeId\", \"pheId\", \"stateId\", and \"freq\"\n",
    "def get_mixehr_metadata(mixehr_df):\n",
    "    meta_df = mixehr_df[[\"typeId\", \"pheId\", \"stateId\"]].drop_duplicates().reset_index(drop=True)\n",
    "    meta_df = meta_df.groupby([\"typeId\", \"pheId\"]).size().to_frame(\"stateCnt\").reset_index()\n",
    "    for typeId in meta_df[\"typeId\"].unique():\n",
    "        meta_df.loc[meta_df[\"typeId\"] == typeId, [\"stateCnt\"]] = meta_df[meta_df[\"typeId\"] == typeId][\"stateCnt\"].max()\n",
    "    return meta_df\n",
    "\n",
    "\n",
    "# Write a df to a .txt file in mixehr format.  \n",
    "# Must specify if it is a metadata file, otherwise\n",
    "# it is assumed to be a data file. \n",
    "def mixehr_df_to_txt(mixehr_df, file_name, metadata=False, prepend_n_patients=False):\n",
    "    try: os.remove(file_name)\n",
    "    except OSError: pass\n",
    "    with open(file_name, 'a') as f:\n",
    "        if not metadata and prepend_n_patients:\n",
    "            # The first line is always the number of patients for some reason\n",
    "            f.write(str(mixehr_df[\"patientId\"].nunique()) + \"\\n\")\n",
    "        np.savetxt(f, mixehr_df.to_numpy(), delimiter=' ', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16fedb-9f44-4004-ab5a-20ba03f998c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More helper functions (for filtering out common values with inflection points)\n",
    "\n",
    "# Get inflection point from frequency values.\n",
    "# IMPORTANT NOTE: the array \"arr\" MUST be sorted in descending order. \n",
    "def get_inflection_point(arr, plotlabel=\"\", plot=True, deriv=1, sg_window=31, sg_order=2, n_pts=1, rare_ind=None):\n",
    "    \n",
    "    # Smooth the curve using a Savitzky-Golay filter\n",
    "    smoothed = savgol_filter(arr, window_length=sg_window, polyorder=sg_order, mode='interp')\n",
    "\n",
    "    if deriv == 1:\n",
    "        # Calculate 1st derivative of smoothed curve\n",
    "        smoothed_1st_deriv = savgol_filter(arr, window_length=sg_window, polyorder=sg_order, mode='interp', deriv=1)\n",
    "        \n",
    "        # Find \"inflection point\" based on a heuristic using the gradient\n",
    "        try: maxi = arr[0]\n",
    "        except KeyError: maxi = arr.iloc[0]\n",
    "        infl_slope = -maxi/(0.5*len(arr))\n",
    "        inflection_ind = np.where(smoothed_1st_deriv > infl_slope)[0][0] + 1\n",
    "        inflection_val = smoothed[inflection_ind]\n",
    "    elif deriv == 2:\n",
    "        # Calculate second derivative of smoothed curve\n",
    "        smoothed_second_deriv = savgol_filter(arr, window_length=sg_window, polyorder=sg_order, mode='interp', deriv=2)\n",
    "\n",
    "        # Find inflection point where second derivative of smoothed curve is near zero\n",
    "        inflection_ind = np.where(np.abs(smoothed_second_deriv) <= 1e-3)[0][0] + 1\n",
    "        inflection_val = smoothed[inflection_ind]\n",
    "    else:\n",
    "        raise ValueError(\"deriv must be 1 or 2\")\n",
    "        \n",
    "    if plot:\n",
    "        plt.figure(figsize=(3,2))\n",
    "        x = np.arange(len(arr), dtype=float)\n",
    "        plt.plot(x, arr/n_pts, label=\"Raw frequencies\")\n",
    "        plt.plot(x, smoothed/n_pts, \"--\", label=\"Savitzky-Golay\")\n",
    "        plt.plot([0, len(arr)], [inflection_val/n_pts, inflection_val/n_pts], label=\"Inflection\")\n",
    "        plt.plot([rare_ind, rare_ind], [0, 0.2*(arr.max()/n_pts)], label=\"Rarity cutoff\")\n",
    "        plt.legend()\n",
    "        plt.title(plotlabel.lower() + \" freqs\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plotlabel.lower() + \"_inflection.png\")\n",
    "        plt.show()\n",
    "        \n",
    "    return inflection_val, inflection_ind\n",
    "\n",
    "\n",
    "# Given a dataframe with a column of interest (string 'col') (e.g. ICD-9 codes), \n",
    "# Generate a bidirectional dictionary (bidict) mapping from integers to the actual value. \n",
    "# If remove_common is True, remove the most common values using the frequency curve inflection point\n",
    "# (similar to supplementary Figure 26 in original MixEHR paper)\n",
    "def unique_mapping(df, col, remove_common=True, remove_rare=True, rare_k=3, plot=True, deriv=1, sg_window=31, sg_order=2):\n",
    "    vcounts = df.groupby(col)[\"SUBJECT_ID\"].nunique().sort_values(ascending=False)\n",
    "    \n",
    "    if remove_common or remove_rare:\n",
    "        \n",
    "        # Find out how many rare items we'll remove, for the plots\n",
    "        rare_ind = np.where(vcounts < rare_k)[0][0]\n",
    "        \n",
    "        # Remove common values based on \"inflection point\" of frequency distribution\n",
    "        if remove_common:\n",
    "            inflection_val, inflection_ind = get_inflection_point(vcounts, plotlabel=col, sg_window=sg_window, sg_order=sg_order, n_pts=df[\"SUBJECT_ID\"].nunique(), rare_ind=rare_ind)\n",
    "            len_before = len(vcounts)\n",
    "            vcounts = vcounts[inflection_ind:]\n",
    "            print(len_before - len(vcounts), \"common\", col, \"values removed from the original set of\", len_before)\n",
    "\n",
    "        # Remove rare values that occur in fewer than k patients.\n",
    "        if remove_rare:\n",
    "            rare_ind = np.where(vcounts < rare_k)[0][0]\n",
    "            len_before = len(vcounts)\n",
    "            vcounts = vcounts[:rare_ind]\n",
    "            print(len_before - len(vcounts), \"rare\", col, \"values removed from the original set of\", len_before)\n",
    "            \n",
    "        keep = vcounts.index.tolist()\n",
    "        orig_len = len(df)\n",
    "        df = df[df[col].isin(keep)]\n",
    "        print(orig_len - len(df), \"rows with common or rare\", col, \"values removed of the original\", orig_len, \"rows\")\n",
    "        \n",
    "    else:\n",
    "        keep = vcounts.index.tolist()\n",
    "\n",
    "    # Make bidict, a bidirectional dictionary\n",
    "    mapping = bidict(dict(zip(range(1, len(keep)+1), keep)))\n",
    "    \n",
    "    df[\"pheId\"] = df[col].map(mapping.inverse)\n",
    "    \n",
    "    return mapping, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb11554-d2ea-41ec-badd-705bcbb13a33",
   "metadata": {},
   "source": [
    "# Train/test/vali splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7eeb6e-dc10-4b64-91db-b49faac88045",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train-vali-test split on subject_id\n",
    "\n",
    "import random\n",
    "\n",
    "# If the pickled lists of subject and training ids are available, just load them from pickle\n",
    "all_files_found = True\n",
    "for idtype in (\"subj\", \"hadm\"):\n",
    "    for idset in (\"train\", \"vali\", \"test\"):\n",
    "        filename = idset + \"_\" + idtype + \"_ids_list.pkl\"\n",
    "        if os.path.isfile(filename): \n",
    "            exec(idset + \"_\" + idtype + \"_ids = pickle.load(open('\" + filename + \"', 'rb'))\")\n",
    "        else:\n",
    "            all_files_found = False\n",
    "\n",
    "if all_files_found:\n",
    "    print(\"Train, vali, and test ids loaded from pickled lists\")\n",
    "else: \n",
    "    random.seed(0)\n",
    "\n",
    "    # Get our subject IDs from trajectories\n",
    "    trajectories = pd.read_csv(\"./../../trajectories.csv\")\n",
    "    trajectories[\"SUBJECT_ID\"] = trajectories[\"subject_id\"]\n",
    "    trajectories[\"HADM_ID\"] = trajectories[\"hadm_id\"]\n",
    "    \n",
    "    \n",
    "    # Only keep icu stays 24 hours or longer\n",
    "    icustays = pd.read_csv(\"./../../ICUSTAYS.csv\")\n",
    "    icustays = icustays[icustays[\"LOS\"] >= 1.5] # Must have stayed at least 36 hours\n",
    "    trajectories = trajectories[trajectories[\"HADM_ID\"].isin(icustays[\"HADM_ID\"])]\n",
    "\n",
    "    trajectories = only_one_hadm_per_subj(trajectories)\n",
    "    subj_ids = list(trajectories[\"SUBJECT_ID\"].unique())\n",
    "    \n",
    "    # Remove a few very specific patients\n",
    "    # (enough data for some conditions but not others, causes dimensionality issues)\n",
    "    remove_these_patients = [64523, 41408, 4064, 18818, 2148, 19980, 6256, 86193, 59762, 886, 19872, 13437, 73833, 14469]\n",
    "    for pt in remove_these_patients:\n",
    "        try:\n",
    "            subj_ids.remove(pt)\n",
    "        except ValueError:\n",
    "            print(\"Subj ID\", pt, \"not found\")\n",
    "\n",
    "    # Randomly select training, test, vali sets\n",
    "    random.shuffle(subj_ids)\n",
    "    train_frac = 0.8\n",
    "    vali_frac = 0.1\n",
    "    test_frac = 0.1\n",
    "    train_subj_ids = subj_ids[0:int(train_frac*len(subj_ids))]\n",
    "    vali_subj_ids = subj_ids[int(train_frac*len(subj_ids)):int((train_frac+vali_frac)*len(subj_ids))]\n",
    "    test_subj_ids = subj_ids[int((train_frac+vali_frac)*len(subj_ids)):]\n",
    "\n",
    "    train_subj_ids.sort()\n",
    "    vali_subj_ids.sort()\n",
    "    test_subj_ids.sort()\n",
    "\n",
    "    with open(\"train_subj_ids_list.pkl\", 'wb') as file: pickle.dump(train_subj_ids, file)\n",
    "    with open(\"vali_subj_ids_list.pkl\", 'wb') as file: pickle.dump(vali_subj_ids, file)\n",
    "    with open(\"test_subj_ids_list.pkl\", 'wb') as file: pickle.dump(test_subj_ids, file)\n",
    "\n",
    "    train_hadm_ids = trajectories[trajectories[\"SUBJECT_ID\"].isin(train_subj_ids)][\"HADM_ID\"].unique().tolist()\n",
    "    vali_hadm_ids = trajectories[trajectories[\"SUBJECT_ID\"].isin(vali_subj_ids)][\"HADM_ID\"].unique().tolist()\n",
    "    test_hadm_ids = trajectories[trajectories[\"SUBJECT_ID\"].isin(test_subj_ids)][\"HADM_ID\"].unique().tolist()\n",
    "\n",
    "    train_hadm_ids.sort()\n",
    "    vali_hadm_ids.sort()\n",
    "    test_hadm_ids.sort()\n",
    "    \n",
    "    with open(\"train_hadm_ids_list.pkl\", 'wb') as file: pickle.dump(train_hadm_ids, file)\n",
    "    with open(\"vali_hadm_ids_list.pkl\", 'wb') as file: pickle.dump(vali_hadm_ids, file)\n",
    "    with open(\"test_hadm_ids_list.pkl\", 'wb') as file: pickle.dump(test_hadm_ids, file)\n",
    "    \n",
    "print(\"N patients in training set: \", len(train_subj_ids))\n",
    "print(\"N patients in validation set: \", len(vali_subj_ids))\n",
    "print(\"N patients in test set: \", len(test_subj_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd5a4e-35e3-41d7-868d-55bf5ca96760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "assert len(set(train_hadm_ids)) == len(train_hadm_ids)\n",
    "assert len(set(vali_hadm_ids)) == len(vali_hadm_ids)\n",
    "assert len(set(test_hadm_ids)) == len(test_hadm_ids)\n",
    "\n",
    "assert set(train_hadm_ids).isdisjoint(set(test_hadm_ids))\n",
    "assert set(train_hadm_ids).isdisjoint(set(vali_hadm_ids))\n",
    "assert set(vali_hadm_ids).isdisjoint(set(test_hadm_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191865db-e549-4041-9e13-801e45f9a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# icustays = only_these_subj_and_hadm_ids(pd.read_csv(\"./../ICUSTAYS.csv\"), train_subj_ids, train_hadm_ids)\n",
    "# icustays[\"intime_dt\"] = icustays.apply(lambda row: datetime.strptime(row[\"INTIME\"], '%Y-%m-%d %H:%M:%S'), axis=1)\n",
    "\n",
    "# icustays[\"los_delta\"] = icustays.apply(lambda row: (datetime.strptime(row[\"OUTTIME\"], '%Y-%m-%d %H:%M:%S') - datetime.strptime(row[\"INTIME\"], '%Y-%m-%d %H:%M:%S')).total_seconds()/(60*60*24), axis=1)\n",
    "\n",
    "# plt.hist(icustays[\"LOS\"], bins=500)\n",
    "# plt.xlim([0, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043ca16-d102-4b39-97d2-971b7d140893",
   "metadata": {},
   "source": [
    "# ICD Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e722d-7abd-4c71-ae57-1b92a93e39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICD codes (both diagnoses and procedures)\n",
    "\n",
    "icd_diag = pd.read_csv(\"./../../MIMIC_III_DIAGNOSES_ICD.csv\")\n",
    "icd_proc = pd.read_csv(\"./../../PROCEDURES_ICD.csv\")\n",
    "\n",
    "icds = pd.concat([icd_diag, icd_proc])\n",
    "icds[\"patientId\"] = icds[\"SUBJECT_ID\"]\n",
    "icds[\"typeId\"] = 1\n",
    "icds[\"stateId\"] = 1 # ICD codes are simply marked \"present\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac32c1-d99d-4516-995f-06b911be2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICD codes for training set\n",
    "\n",
    "if PREPARE_TRAINING_DATA:\n",
    "    icds_subset = only_these_subj_and_hadm_ids(icds, train_subj_ids, train_hadm_ids)\n",
    "\n",
    "    icds_mapper, icds_subset = unique_mapping(icds_subset, \"ICD9_CODE\", remove_common=True, remove_rare=True, plot=True)\n",
    "    with open(\"icds_mapper.pkl\", 'wb') as file:\n",
    "        pickle.dump(icds_mapper, file)\n",
    "\n",
    "    icds_subset_mixehr = get_mixehr_format_with_freqs(icds_subset)\n",
    "    icds_subset_mixehr.to_csv(\"train_icds_mixehr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6ee71-e528-4244-83fc-fe1b0fb6629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICD codes for vali and test sets\n",
    "\n",
    "for idset in [\"vali\", \"test\"]:\n",
    "    subj_ids = eval(idset + \"_subj_ids\")\n",
    "    hadm_ids = eval(idset + \"_hadm_ids\")\n",
    "    icds_subset = only_these_subj_and_hadm_ids(icds, subj_ids, hadm_ids)\n",
    "    icds_mapper = pickle.load(open(\"icds_mapper.pkl\", 'rb'))\n",
    "    icds_subset = prepare_vali_test_sets(icds_subset, icds_mapper, \"ICD9_CODE\", idset)\n",
    "    icds_subset_mixehr = get_mixehr_format_with_freqs(icds_subset)\n",
    "    icds_subset_mixehr.to_csv(idset + \"_icds_mixehr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9410207-cc33-4f81-ade9-4086536ae396",
   "metadata": {},
   "source": [
    "# Prescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40ef44-abea-4e76-bbf2-24be8dbc1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prescriptions\n",
    "\n",
    "# I make these simplifications to routes of administration: \n",
    "# All oral and gastric tube administration routes are denoted \"PO/GT\" without distinction\n",
    "# All duodenal routes (e.g. nasoduodenal) are marked as \"PO/GT\", same as gastric routes\n",
    "# All intraocular drugs are \"OU\" (no left vs right vs both eyes etc)\n",
    "# All intraaural drugs are \"EAR\" (no left vs right vs both ears etc)\n",
    "route_dict = {\n",
    "    \"PO\": \"PO/GT\", # By mouth or gastric tube\n",
    "    \"ORAL\": \"PO/GT\", \n",
    "    \"PO/NG\": \"PO/GT\",\n",
    "    \"PO/OG\": \"PO/GT\",\n",
    "    \"NG\": \"PO/GT\",\n",
    "    \"OG\": \"PO/GT\",\n",
    "    \"PO OR ENTERAL TUBE\": \"PO/GT\",\n",
    "    \"G TUBE\": \"PO/GT\",\n",
    "    \"ND\": \"PO/GT\",\n",
    "    \"PO OR ENTERAL TUBE\": \"PO/GT\",\n",
    "    \"ENTERAL TUBE ONLY ? NOT ORAL\": \"PO/GT\",\n",
    "    \"RIGHT EYE\": \"OU\", # Ocular/eye\n",
    "    \"LEFT EYE\": \"OU\",\n",
    "    \"BOTH EYES\": \"OU\",\n",
    "    \"OS\": \"OU\",\n",
    "    \"OD\": \"OU\",\n",
    "    \"AD\": \"EAR\",\n",
    "    \"AU\": \"EAR\",\n",
    "    \"AS\": \"EAR\",\n",
    "    \"BOTH EARS\": \"EAR\",\n",
    "    \"LEFT EAR\": \"EAR\",\n",
    "    \"RIGHT EAR\": \"EAR\",\n",
    "    \"NAS\": \"NU\", # Nasal\n",
    "    \"NS\": \"NU\",\n",
    "    \"IN\": \"NU\"\n",
    "}\n",
    "\n",
    "prescrips = pd.read_csv(\"./../../PRESCRIPTIONS.csv\")\n",
    "prescrips[\"patientId\"] = prescrips[\"SUBJECT_ID\"]\n",
    "prescrips[\"typeId\"] = 2\n",
    "prescrips[\"stateId\"] = 1 # Prescriptions are simply marked \"present\"\n",
    "\n",
    "prescrips[\"ROUTE\"] = prescrips[\"ROUTE\"].fillna(\"UNSPECIFIED\")\n",
    "\n",
    "# Simplify the route abbreviations - e.g., for this cardiac mixehr, assume anything oral/gastric/intraduodenal is the same\n",
    "prescrips[\"simplified_route\"] = prescrips[\"ROUTE\"].map(route_dict).fillna(prescrips[\"ROUTE\"]).astype(str)\n",
    "\n",
    "# Concatenate \"DRUG\" column with simplified route column\n",
    "prescrips[\"drug_id\"] = prescrips[\"DRUG\"].astype(str) + \"-\" + prescrips[\"simplified_route\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6df6d-af98-441f-a97f-15c1e3c90eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prescriptions for training set\n",
    "\n",
    "if PREPARE_TRAINING_DATA:\n",
    "    prescrips_subset = only_these_subj_and_hadm_ids(prescrips, train_subj_ids, train_hadm_ids)\n",
    "\n",
    "    prescrips_subset = within_x_secs_of_admit(\n",
    "        prescrips_subset, \n",
    "        3600*24, \n",
    "        only_these_subj_and_hadm_ids(pd.read_csv(\"./../../ICUSTAYS.csv\"), train_subj_ids, train_hadm_ids),\n",
    "        col_name=\"STARTDATE\"\n",
    "    )\n",
    "\n",
    "    prescrips_mapper, prescrips_subset = unique_mapping(prescrips_subset, \"drug_id\", remove_common=True, remove_rare=True, plot=True)\n",
    "    with open(\"prescrips_mapper.pkl\", 'wb') as file:\n",
    "        pickle.dump(prescrips_mapper, file)\n",
    "\n",
    "    prescrips_subset_mixehr = get_mixehr_format_with_freqs(prescrips_subset)\n",
    "    prescrips_subset_mixehr.to_csv(\"train_prescrips_mixehr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad8738-799d-418d-96c6-094b7c4930d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prescriptions for vali and test sets\n",
    "\n",
    "for idset in [\"vali\", \"test\"]:\n",
    "    subj_ids = eval(idset + \"_subj_ids\")\n",
    "    hadm_ids = eval(idset + \"_hadm_ids\")\n",
    "    prescrips_subset = only_these_subj_and_hadm_ids(prescrips, subj_ids, hadm_ids)\n",
    "\n",
    "    prescrips_subset = within_x_secs_of_admit(\n",
    "        prescrips_subset, \n",
    "        3600*24, \n",
    "        only_these_subj_and_hadm_ids(pd.read_csv(\"./../../ICUSTAYS.csv\"), subj_ids, hadm_ids),\n",
    "        col_name=\"STARTDATE\"\n",
    "    )\n",
    "    \n",
    "    prescrips_mapper = pickle.load(open(\"prescrips_mapper.pkl\", 'rb'))\n",
    "    prescrips_subset = prepare_vali_test_sets(prescrips_subset, prescrips_mapper, \"drug_id\", idset)\n",
    "    prescrips_subset_mixehr = get_mixehr_format_with_freqs(prescrips_subset)\n",
    "    prescrips_subset_mixehr.to_csv(idset + \"_prescrips_mixehr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98690cee-6a7f-4931-9124-867a9fa8650d",
   "metadata": {},
   "source": [
    "# Lab tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1e472-7bef-4583-aebf-ae6d84ae0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB TESTS\n",
    "\n",
    "labs = pd.read_csv(\"./../../LABEVENTS.csv\")\n",
    "labs[\"patientId\"] = labs[\"SUBJECT_ID\"]\n",
    "labs[\"typeId\"] = 3\n",
    "\n",
    "# For some reason, lots of missing HADM_IDs in the labs\n",
    "# They could mostly not be recovered from ICUSTAYS.csv\n",
    "# using SUBJECT_ID and timestamps. \n",
    "labs = labs[labs[\"HADM_ID\"].notna()]\n",
    "\n",
    "labs[\"lab_id\"] = labs[\"ITEMID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5227607-7ac0-437f-857d-a85644d716f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab tests for training set\n",
    "\n",
    "if PREPARE_TRAINING_DATA:\n",
    "    labs_subset = only_these_subj_and_hadm_ids(labs, train_subj_ids, train_hadm_ids)\n",
    "    \n",
    "    labs_subset = within_x_secs_of_admit(\n",
    "        labs_subset, \n",
    "        3600*24, \n",
    "        only_these_subj_and_hadm_ids(pd.read_csv(\"./../../ICUSTAYS.csv\"), train_subj_ids, train_hadm_ids)\n",
    "    )\n",
    "    \n",
    "    # 0 for nan or \"DELTA\" value in FLAG column, 1 for \"abnormal\"\n",
    "    labs_subset[\"stateId\"] = labs_subset.apply(lambda row: 1 if row[\"FLAG\"] == \"abnormal\" else 0, axis=1)\n",
    "\n",
    "    labs_mapper, labs_subset = unique_mapping(labs_subset, \"lab_id\", remove_common=True, remove_rare=True, plot=True, sg_window=51)\n",
    "    with open(\"labs_mapper.pkl\", 'wb') as file:\n",
    "        pickle.dump(labs_mapper, file)\n",
    "\n",
    "    labs_subset_mixehr = get_mixehr_format_with_freqs(labs_subset)\n",
    "    labs_subset_mixehr.to_csv(\"train_labs_mixehr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74799e09-0571-4dad-b3c7-147fd7193f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs for vali and test sets\n",
    "\n",
    "for idset in [\"vali\", \"test\"]:\n",
    "    subj_ids = eval(idset + \"_subj_ids\")\n",
    "    hadm_ids = eval(idset + \"_hadm_ids\")\n",
    "    labs_subset = only_these_subj_and_hadm_ids(labs, subj_ids, hadm_ids)\n",
    "    \n",
    "    labs_subset = within_x_secs_of_admit(\n",
    "        labs_subset, \n",
    "        3600*24, \n",
    "        only_these_subj_and_hadm_ids(pd.read_csv(\"./../../ICUSTAYS.csv\"), subj_ids, hadm_ids)\n",
    "    )\n",
    "    \n",
    "    # 0 for nan or \"DELTA\" value in FLAG column, 1 for \"abnormal\"\n",
    "    labs_subset[\"stateId\"] = labs_subset.apply(lambda row: 1 if row[\"FLAG\"] == \"abnormal\" else 0, axis=1)\n",
    "    \n",
    "    labs_mapper = pickle.load(open(\"labs_mapper.pkl\", 'rb'))\n",
    "    labs_subset = prepare_vali_test_sets(labs_subset, labs_mapper, \"lab_id\", idset)\n",
    "    labs_subset_mixehr = get_mixehr_format_with_freqs(labs_subset)\n",
    "    labs_subset_mixehr.to_csv(idset + \"_labs_mixehr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad45ad33-46c6-489b-9767-6f90a58a86d8",
   "metadata": {},
   "source": [
    "# Clinical notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c131d7-f90f-46ae-a5dc-9036144c634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up NLTK for clinical notes\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_tokens(text, stop_words, punctuation, restricted_dict=None):\n",
    "    # Lowercase then tokenize, after removing ** wherever it appears (common around dates etc in MIMIC notes)\n",
    "    word_tokens = word_tokenize(text.replace(\"**\", \"\").lower()) \n",
    "    return [\n",
    "        token for token in word_tokens \n",
    "        if token not in stop_words \n",
    "        and ((restricted_dict is None) or (token in restricted_dict)) # Keep only tokens in a set list\n",
    "    ]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(punctuation)\n",
    "punctuation.update(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba3fb5-e38d-4fa4-a378-79f22b03e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save separate note files for training, vali, and test notes\n",
    "\n",
    "if MAKE_SEPARATE_NOTE_CSVS:\n",
    "    notes = pd.read_csv(\"./../../NOTEEVENTS.csv\")\n",
    "\n",
    "    notes_train = only_these_subj_and_hadm_ids(notes, train_subj_ids, train_hadm_ids)\n",
    "    notes_train.to_csv(\"TRAIN_NOTEEVENTS.csv\")\n",
    "\n",
    "    notes_vali = only_these_subj_and_hadm_ids(notes, vali_subj_ids, vali_hadm_ids)\n",
    "    notes_vali.to_csv(\"VALI_NOTEEVENTS.csv\")\n",
    "\n",
    "    notes_test = only_these_subj_and_hadm_ids(notes, test_subj_ids, test_hadm_ids)\n",
    "    notes_test.to_csv(\"TEST_NOTEEVENTS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b8c12-5746-48a2-8e23-533d9cc8da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary from clinical notes training set (takes about 40 minutes)\n",
    "from tqdm import tqdm\n",
    "\n",
    "if BUILD_NOTES_DICTIONARY: \n",
    "    notes = pd.read_csv(\"./TRAIN_NOTEEVENTS.csv\", usecols=[\"SUBJECT_ID\", \"HADM_ID\", \"CHARTTIME\", \"TEXT\"])\n",
    "    num_notes = len(notes)\n",
    "    notes = only_these_subj_and_hadm_ids(notes, train_subj_ids, train_hadm_ids)\n",
    "    assert num_notes == len(notes), \"Inconsistent selection of training set!\"\n",
    "    \n",
    "    notes = within_x_secs_of_admit(\n",
    "        notes, \n",
    "        3600*24, \n",
    "        only_these_subj_and_hadm_ids(pd.read_csv(\"./../../ICUSTAYS.csv\"), train_subj_ids, train_hadm_ids)\n",
    "    )\n",
    "\n",
    "    # Initialize an empty FreqDist\n",
    "    fdist = nltk.FreqDist()\n",
    "\n",
    "    for pt in tqdm(train_subj_ids):\n",
    "        pt_notes = notes[notes[\"SUBJECT_ID\"] == pt]\n",
    "        concat_notes = \" \".join(pt_notes[\"TEXT\"])\n",
    "        tokens = get_tokens(concat_notes, stop_words, punctuation)\n",
    "        fdist.update(set(tokens))\n",
    "\n",
    "    word_freq_df = pd.DataFrame(fdist.items(), columns=['word', 'frequency'])\n",
    "    word_freq_df = word_freq_df.sort_values(by=[\"frequency\"], ascending=False).reset_index(drop=True)\n",
    "    word_freq_df.to_csv(\"train_notes_word_freqs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94524018-09b0-4c89-9870-f0228f499285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some filtering on the extracted dictionary and make a bidirectional mapping\n",
    "\n",
    "if BUILD_NOTES_DICTIONARY:\n",
    "    word_freq_df = pd.read_csv(\"train_notes_word_freqs.csv\")\n",
    "    word_freq_df = word_freq_df.sort_values(by=[\"frequency\"], ascending=False).reset_index(drop=True)\n",
    "    word_freq_df.to_csv(\"train_notes_word_freqs.csv\", index=False)\n",
    "    \n",
    "    # Only keep words that occur for at least k patients\n",
    "    k = 3\n",
    "    length_before = len(word_freq_df)\n",
    "    word_freq_df = word_freq_df[word_freq_df[\"frequency\"] >= k]\n",
    "    print(length_before-len(word_freq_df), \"of\", length_before, \"words removed because they occurred for less than\", k, \"patients\")\n",
    "\n",
    "    # Remove any words that contain punctuation or numbers\n",
    "    length_before = len(word_freq_df)\n",
    "    word_freq_df[\"nopunc\"] = word_freq_df.apply(lambda row: set(str(row[\"word\"])).isdisjoint(punctuation), axis=1)\n",
    "    word_freq_df = word_freq_df[word_freq_df[\"nopunc\"]].drop([\"nopunc\"], axis=1)\n",
    "    print(length_before-len(word_freq_df), \"of\", length_before, \"words removed because they contained numbers or punctuation\")\n",
    "\n",
    "    # Remove common words based on inflection point\n",
    "    length_before = len(word_freq_df)\n",
    "    inflection_val, inflection_ind = get_inflection_point(word_freq_df[\"frequency\"], plotlabel=\"Notes word\")\n",
    "    word_freq_df = word_freq_df[inflection_ind:].reset_index(drop=True)\n",
    "    print(length_before-len(word_freq_df), \"common words removed from the original set of\", length_before)\n",
    "\n",
    "    word_freq_df.to_csv(\"cleaned_notes_dict.csv\", index=False)\n",
    "\n",
    "    # Make bidict, a bidirectional dictionary\n",
    "    note_words_mapper = bidict(dict(zip(range(1, len(word_freq_df)+1), word_freq_df[\"word\"].tolist())))\n",
    "    with open(\"note_words_mapper.pkl\", 'wb') as file:\n",
    "        pickle.dump(note_words_mapper, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3e7d6-79f3-4d4e-95a8-4f3db0229f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the final dataframe(s) for clinical notes (takes about 10 minutes for training set)\n",
    "from tqdm import tqdm\n",
    "\n",
    "idsets = []\n",
    "if PREPARE_TRAINING_DATA:\n",
    "    idsets.append(\"train\")\n",
    "if PREPARE_VALI_TEST_DATA:\n",
    "    idsets.extend([\"vali\", \"test\"])\n",
    "\n",
    "note_words_mapper = pickle.load(open(\"note_words_mapper.pkl\", 'rb'))    \n",
    "\n",
    "for idset in idsets:\n",
    "    notes = pd.read_csv(\"./\" + idset.upper() + \"_NOTEEVENTS.csv\", usecols=[\"SUBJECT_ID\", \"HADM_ID\", \"CHARTTIME\", \"TEXT\"])\n",
    "    num_notes = len(notes)\n",
    "    subj_ids = eval(idset + \"_subj_ids\")\n",
    "    hadm_ids = eval(idset + \"_hadm_ids\")\n",
    "    notes = only_these_subj_and_hadm_ids(notes, subj_ids, hadm_ids)\n",
    "    assert num_notes == len(notes), \"Inconsistent selection of \" + idset + \" set!\"\n",
    "    \n",
    "    notes = within_x_secs_of_admit(\n",
    "        notes, \n",
    "        3600*24, \n",
    "        only_these_subj_and_hadm_ids(pd.read_csv(\"./../../ICUSTAYS.csv\"), subj_ids, hadm_ids)\n",
    "    )\n",
    "\n",
    "    cleaned_notes_dict = set(pd.read_csv(\"cleaned_notes_dict.csv\")[\"word\"].tolist())\n",
    "\n",
    "    # Iterate over each patient in the training, vali, or test DataFrame\n",
    "    # ASSUME ONLY ONE HADM PER PATIENT!\n",
    "    pt_dfs = []\n",
    "    for pt in tqdm(subj_ids):\n",
    "        pt_notes = notes[notes[\"SUBJECT_ID\"] == pt]\n",
    "        concat_notes = \" \".join(pt_notes[\"TEXT\"])\n",
    "        tokens = get_tokens(concat_notes, stop_words, punctuation, restricted_dict=cleaned_notes_dict)\n",
    "        fdist = nltk.FreqDist(tokens)\n",
    "        pt_df = pd.DataFrame(fdist.items(), columns=['word', 'freq'])\n",
    "        pt_df[\"patientId\"] = pt\n",
    "        pt_dfs.append(pt_df)\n",
    "\n",
    "    note_words_mixehr = pd.concat(pt_dfs)\n",
    "\n",
    "    note_words_mixehr[\"typeId\"] = 4\n",
    "    note_words_mixehr[\"pheId\"] = note_words_mixehr.apply(lambda row: note_words_mapper.inverse[row[\"word\"]], axis=1)\n",
    "    note_words_mixehr[\"stateId\"] = 1\n",
    "\n",
    "    note_words_mixehr = note_words_mixehr[[\"patientId\", \"typeId\", \"pheId\", \"stateId\", \"freq\"]]\n",
    "\n",
    "    note_words_mixehr.to_csv(idset + \"_note_words_mixehr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808af140-070e-41df-8801-c85293d08ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used this to view some words with low/uncommon frequencies to decide on a cutoff\n",
    "\n",
    "# word_freq_df = pd.read_csv(\"notes_word_freqs.csv\")\n",
    "# print(len(word_freq_df))\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# print(len(word_freq_df[word_freq_df[\"frequency\"] == 2]))\n",
    "# word_freq_df[word_freq_df[\"frequency\"] == 2].iloc[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6313eb4-1028-4662-8276-939acc6039bc",
   "metadata": {},
   "source": [
    "# ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e690650b-7bb8-49a4-8511-b2d3e55cb0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = pd.read_csv(\"./../../trajectories.csv\")\n",
    "print(\"Mean number of HADM_IDs per patient:\", traj.groupby(\"subject_id\")[\"hadm_id\"].nunique().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7699c027-6e12-43ce-acd6-077584b87612",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = pd.read_csv(\"./../../trajectories_with_features.csv\")\n",
    "\n",
    "# Define which ecg features we are using\n",
    "misc_features = [\"ibi\", \"bpm\", \"sdnn\", \"sdsd\", \"rmssd\", \"pnn50\", \"pnn20\"]\n",
    "cols_list = [col for col in list(ecg.columns.values) if (\n",
    "    \"full_\" in col or \"swt_\" in col or \"HRV_\" in col or col in misc_features\n",
    ")]\n",
    "\n",
    "# Remove features with 100% missingness\n",
    "ecg = ecg.replace([np.inf, -np.inf], np.nan)\n",
    "ecg_features = []\n",
    "for col in cols_list:\n",
    "    prop_missing = ecg[col].isnull().sum()/len(ecg.index)\n",
    "    if prop_missing < 1:\n",
    "        ecg_features.append(col)\n",
    "ecg_features.remove(\"full_waveform_duration\") # This variable probably doesn't make sense as a feature\n",
    "\n",
    "ecg = ecg.rename(columns={\"subject_id\": \"SUBJECT_ID\", \"hadm_id\": \"HADM_ID\"})[[\"SUBJECT_ID\", \"HADM_ID\", \"start_hr\", \"ts_idx\"] + ecg_features]\n",
    "\n",
    "idsets = []\n",
    "if PREPARE_TRAINING_DATA:\n",
    "    idsets.append(\"train\")\n",
    "if PREPARE_VALI_TEST_DATA:\n",
    "    idsets.extend([\"vali\", \"test\"])\n",
    "    \n",
    "# If we're preparing the training data, we are generating the quantiles. \n",
    "# For vali and test data, we use the quantile values drawn from the training set. \n",
    "if not PREPARE_TRAINING_DATA and PREPARE_VALI_TEST_DATA:\n",
    "    quantiles_dict = pickle.load(open(\"ecg_quantiles_dict.pkl\", 'rb'))\n",
    "else:\n",
    "    quantiles_dict = {}\n",
    "    \n",
    "for idset in idsets:\n",
    "    subj_ids = eval(idset + \"_subj_ids\")\n",
    "    hadm_ids = eval(idset + \"_hadm_ids\")\n",
    "    ecg_subset = only_these_subj_and_hadm_ids(ecg, subj_ids, hadm_ids)\n",
    "    \n",
    "    # Keep only trajectories that start within the first 12 hours of ICU stay\n",
    "    ecg_subset = ecg_subset[ecg_subset[\"start_hr\"] <= 12]\n",
    "\n",
    "    # Prepare quantile features - i.e., \"True\" if the feature (e.g. ECG standard deviation in a one-hour segment)\n",
    "    # is in some \"extreme\" quantile (e.g. 25th or 75th percentile)\n",
    "    quantile_features = []\n",
    "    for feat in ecg_features:\n",
    "        if PREPARE_TRAINING_DATA and idset == \"train\":\n",
    "            quantiles_dict[feat] = [ecg_subset[feat].quantile(0.25), ecg_subset[feat].quantile(0.75)]\n",
    "        low_quantile_feat_col = feat + \"-low\"\n",
    "        high_quantile_feat_col = feat + \"-high\"\n",
    "        ecg_subset[\"value@\" + low_quantile_feat_col] = ecg_subset[feat] < quantiles_dict[feat][0]\n",
    "        ecg_subset[\"value@\" + high_quantile_feat_col] = ecg_subset[feat] > quantiles_dict[feat][1]\n",
    "        quantile_features.extend([low_quantile_feat_col, high_quantile_feat_col])\n",
    "    ecg_subset = ecg_subset.drop(ecg_features, axis=1)\n",
    "    \n",
    "    if PREPARE_TRAINING_DATA and idset == \"train\":\n",
    "        with open(\"ecg_quantiles_dict.pkl\", 'wb') as file:\n",
    "            pickle.dump(quantiles_dict, file)\n",
    "\n",
    "    # This is the key reshaping function - from \"wide\" format with one column per quantile feature, \n",
    "    # to each instance of any feature having its row in a df with lots and lots of rows. \n",
    "    long_ecg = pd.wide_to_long(ecg_subset, stubnames=\"value\", i=[\"SUBJECT_ID\", \"HADM_ID\", \"ts_idx\"], \n",
    "                               j=\"feature\", sep=\"@\", suffix=\"(\" + \"|\".join(quantile_features) + \")\").reset_index()\n",
    "    \n",
    "    # Keep only instances where the quantiles were exceeded\n",
    "    long_ecg = long_ecg[long_ecg[\"value\"]]\n",
    "\n",
    "    long_ecg[\"patientId\"] = long_ecg[\"SUBJECT_ID\"]\n",
    "    long_ecg[\"typeId\"] = 5 # ecg data designated as type 5\n",
    "    long_ecg[\"stateId\"] = 1 # ecg quantile features are simply marked \"present\"\n",
    "\n",
    "    # Get mapping dictionary from feature names to mixehr integers\n",
    "    if PREPARE_TRAINING_DATA and idset == \"train\":\n",
    "        ecg_quantile_features_mapper = bidict(dict(zip(range(1, len(quantile_features)+1), quantile_features)))\n",
    "        with open(\"ecg_quantile_features_mapper.pkl\", 'wb') as file:\n",
    "            pickle.dump(ecg_quantile_features_mapper, file)\n",
    "    else:\n",
    "        ecg_quantile_features_mapper = pickle.load(open(\"ecg_quantile_features_mapper.pkl\", 'rb'))\n",
    "        \n",
    "    long_ecg[\"pheId\"] = long_ecg[\"feature\"].map(ecg_quantile_features_mapper.inverse)\n",
    "\n",
    "    ecg_quantiles_mixehr = get_mixehr_format_with_freqs(long_ecg)\n",
    "    ecg_quantiles_mixehr.to_csv(idset + \"_ecg_quantiles_mixehr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a80b5-632d-4f06-9bae-3b64b596140e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Combining modalities for mixehr training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259dafc-4c8e-409d-bd3e-8f8ada48afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify patients who are included in some conditions but not others\n",
    "# (modify manually as new conditions are added)\n",
    "for idset in [\"train\", \"vali\", \"test\"]:\n",
    "\n",
    "    dfs = []\n",
    "    dfs.extend([\n",
    "        #pd.read_csv(idset + \"_icds_mixehr.csv\"),\n",
    "        pd.read_csv(idset + \"_prescrips_mixehr.csv\"),\n",
    "        pd.read_csv(idset + \"_labs_mixehr.csv\"),\n",
    "    ])\n",
    "\n",
    "    no_notes_no_waveforms = pd.concat(dfs)\n",
    "\n",
    "    dfs.extend([\n",
    "        pd.read_csv(idset + \"_note_words_mixehr.csv\"),\n",
    "        pd.read_csv(idset + \"_ecg_quantiles_mixehr.csv\"),\n",
    "    ])\n",
    "    \n",
    "    everything = pd.concat(dfs)\n",
    "    \n",
    "    ecg_quantiles_only = pd.read_csv(idset + \"_ecg_quantiles_mixehr.csv\")\n",
    "    \n",
    "    print(\"------------\")\n",
    "    print(idset)\n",
    "    print(no_notes_no_waveforms[\"patientId\"].nunique())\n",
    "    print(everything[\"patientId\"].nunique())\n",
    "    print(ecg_quantiles_only[\"patientId\"].nunique())\n",
    "    \n",
    "    print((everything.apply(set) - no_notes_no_waveforms.apply(set))[\"patientId\"])\n",
    "    print((everything.apply(set) - ecg_quantiles_only.apply(set))[\"patientId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c04fe-019e-4834-998d-16478c3a9485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of the mixehr-formatted csvs from prescriptions, labs, note words, and ecg feature quantiles\n",
    "\n",
    "if COMBINE_DATA:\n",
    "    for idset in [\"train\", \"vali\", \"test\"]:\n",
    "        mixehr_data = pd.concat([\n",
    "            #pd.read_csv(idset + \"_icds_mixehr.csv\"),\n",
    "            pd.read_csv(idset + \"_prescrips_mixehr.csv\"),\n",
    "            pd.read_csv(idset + \"_labs_mixehr.csv\"),\n",
    "            pd.read_csv(idset + \"_note_words_mixehr.csv\"),\n",
    "            pd.read_csv(idset + \"_ecg_quantiles_mixehr.csv\"),\n",
    "        ])\n",
    "\n",
    "        mixehr_data = mixehr_data.sort_values(by=[\"patientId\", \"typeId\", \"pheId\", \"stateId\"])\n",
    "        mixehr_metadata = get_mixehr_metadata(mixehr_data)\n",
    "\n",
    "        mixehr_data.to_csv(idset + \"_mixehr_early_with_ecg_quantiles.csv\", index=False)\n",
    "        mixehr_df_to_txt(mixehr_data, idset + \"_mixehr_early_with_ecg_quantiles.txt\", metadata=False)\n",
    "\n",
    "        if idset == \"train\":\n",
    "            mixehr_metadata.to_csv(idset + \"_mixehr_metadata_early_with_ecg_quantiles.csv\", index=False)\n",
    "            mixehr_df_to_txt(mixehr_metadata, idset + \"_mixehr_metadata_early_with_ecg_quantiles.txt\", metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af5db9-8e13-42ea-81eb-7aa330ca9f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to train mixehr with only ecg feature quantiles\n",
    "\n",
    "if COMBINE_DATA:\n",
    "    for idset in [\"train\", \"vali\", \"test\"]:\n",
    "        mixehr_data = pd.concat([\n",
    "            pd.read_csv(idset + \"_ecg_quantiles_mixehr.csv\"),\n",
    "        ])\n",
    "\n",
    "        mixehr_data = mixehr_data.sort_values(by=[\"patientId\", \"typeId\", \"pheId\", \"stateId\"])\n",
    "        mixehr_metadata = get_mixehr_metadata(mixehr_data)\n",
    "\n",
    "        mixehr_data.to_csv(idset + \"_mixehr_early_only_ecg_quantiles.csv\", index=False)\n",
    "        mixehr_df_to_txt(mixehr_data, idset + \"_mixehr_early_only_ecg_quantiles.txt\", metadata=False)\n",
    "\n",
    "        if idset == \"train\":\n",
    "            mixehr_metadata.to_csv(idset + \"_mixehr_metadata_early_only_ecg_quantiles.csv\", index=False)\n",
    "            mixehr_df_to_txt(mixehr_metadata, idset + \"_mixehr_metadata_early_only_ecg_quantiles.txt\", metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e5d7e-b1aa-42c8-82e6-d57d89ead75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of the mixehr-formatted csvs from prescriptions, labs, and note words\n",
    "# NO WAVEFORMS\n",
    "\n",
    "if COMBINE_DATA:\n",
    "    for idset in [\"train\", \"vali\", \"test\"]:\n",
    "        mixehr_data = pd.concat([\n",
    "            #pd.read_csv(idset + \"_icds_mixehr.csv\"),\n",
    "            pd.read_csv(idset + \"_prescrips_mixehr.csv\"),\n",
    "            pd.read_csv(idset + \"_labs_mixehr.csv\"),\n",
    "            pd.read_csv(idset + \"_note_words_mixehr.csv\"),\n",
    "        ])\n",
    "\n",
    "        mixehr_data = mixehr_data.sort_values(by=[\"patientId\", \"typeId\", \"pheId\", \"stateId\"])\n",
    "        mixehr_metadata = get_mixehr_metadata(mixehr_data)\n",
    "\n",
    "        mixehr_data.to_csv(idset + \"_mixehr_early_no_waveforms.csv\", index=False)\n",
    "        mixehr_df_to_txt(mixehr_data, idset + \"_mixehr_early_no_waveforms.txt\", metadata=False)\n",
    "\n",
    "        if idset == \"train\":\n",
    "            mixehr_metadata.to_csv(idset + \"_mixehr_metadata_early_no_waveforms.csv\", index=False)\n",
    "            mixehr_df_to_txt(mixehr_metadata, idset + \"_mixehr_metadata_early_no_waveforms.txt\", metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9dbb77-b4df-40f6-8ca4-1cd9ae35ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of the mixehr-formatted csvs from prescriptions, and labs\n",
    "# NO WAVEFORMS AND NO NOTES\n",
    "\n",
    "if COMBINE_DATA: \n",
    "    for idset in [\"train\", \"vali\", \"test\"]:\n",
    "        mixehr_data = pd.concat([\n",
    "            #pd.read_csv(idset + \"_icds_mixehr.csv\"),\n",
    "            pd.read_csv(idset + \"_prescrips_mixehr.csv\"),\n",
    "            pd.read_csv(idset + \"_labs_mixehr.csv\"),\n",
    "        ])\n",
    "\n",
    "        mixehr_data = mixehr_data.sort_values(by=[\"patientId\", \"typeId\", \"pheId\", \"stateId\"])\n",
    "        mixehr_metadata = get_mixehr_metadata(mixehr_data)\n",
    "\n",
    "        mixehr_data.to_csv(idset + \"_mixehr_early_no_notes_no_waveforms.csv\", index=False)\n",
    "        mixehr_df_to_txt(mixehr_data, idset + \"_mixehr_early_no_notes_no_waveforms.txt\", metadata=False)\n",
    "\n",
    "        if idset == \"train\":\n",
    "            mixehr_metadata.to_csv(idset + \"_mixehr_metadata_early_no_notes_no_waveforms.csv\", index=False)\n",
    "            mixehr_df_to_txt(mixehr_metadata, idset + \"_mixehr_metadata_early_no_notes_no_waveforms.txt\", metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63698ec1-07bc-4fb9-8c2b-75b0749efaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some stats about mixehr-formatted dataframe (compare to mixehr printouts for debugging)\n",
    "\n",
    "print(\"Df length:\", len(mixehr_data))\n",
    "print(\"Num of typeId types (including labs): \", mixehr_data[\"typeId\"].nunique())\n",
    "phetypes = mixehr_data[mixehr_data[\"typeId\"] != 3][[\"typeId\", \"pheId\"]].groupby([\"typeId\"]).nunique().reset_index().rename(columns={0:'count'})\n",
    "print(\"Num of non-lab pheId types: \", phetypes[\"pheId\"].sum())\n",
    "print(\"Num of lab tests: \", mixehr_data[mixehr_data[\"typeId\"] == 3][\"pheId\"].nunique())\n",
    "print(\"Num of patients: \", mixehr_data[\"patientId\"].nunique())\n",
    "print(\"Max pt ID: \", mixehr_data[\"patientId\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7079dc7-1e2a-431e-84ae-21f6783bf995",
   "metadata": {},
   "source": [
    "# Miscellaneous visualizations etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c37c17-17b2-4287-b1dc-b68a3f60da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = pd.read_csv(\"./../../trajectories_with_features.csv\")\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "matplotlib.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# Define which ecg features we are using\n",
    "misc_features = [\"ibi\", \"bpm\", \"sdnn\", \"sdsd\", \"rmssd\", \"pnn50\", \"pnn20\"]\n",
    "cols_list = [col for col in list(ecg.columns.values) if (\n",
    "    \"full_\" in col or \"swt_\" in col or \"HRV_\" in col or col in misc_features\n",
    ")]\n",
    "\n",
    "# Remove features with 100% missingness\n",
    "ecg = ecg.replace([np.inf, -np.inf], np.nan)\n",
    "ecg_features = []\n",
    "for col in cols_list:\n",
    "    prop_missing = ecg[col].isnull().sum()/len(ecg.index)\n",
    "    if prop_missing < 1:\n",
    "        ecg_features.append(col)\n",
    "ecg_features.remove(\"full_waveform_duration\") # This variable probably doesn't make sense as a feature\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "savelist = [\"HRV_CVI\", \"HRV_pNN20\", \"HRV_pNN50\"]\n",
    "\n",
    "for feat in ecg_features:\n",
    "    print(feat)\n",
    "    plt.figure(figsize=(4,2))\n",
    "    most_of_dist = ecg[(ecg[feat] > ecg[feat].median() - 3*ecg[feat].std()) & (ecg[feat] < ecg[feat].median() + 3*ecg[feat].std())]\n",
    "    plt.hist(most_of_dist[feat], bins=50, label=\"hist\", color=\"dodgerblue\", density=True)\n",
    "    plt.xlim(ecg[feat].mean() - 3*ecg[feat].std(), ecg[feat].mean() + 3*ecg[feat].std())\n",
    "    plt.axvline(ecg[feat].mean(), color=\"red\", label=\"mean\")\n",
    "    plt.axvline(ecg[feat].median(), color=\"green\", label=\"median\")\n",
    "    #print(ecg[feat].quantile(0.1))\n",
    "    plt.axvline(ecg[feat].quantile(0.25), color=\"k\", label=\"quantile\")\n",
    "    plt.axvline(ecg[feat].quantile(0.75), color=\"k\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.title(feat)\n",
    "    plt.tight_layout()\n",
    "    if feat in savelist:\n",
    "        plt.savefig(feat + \".png\", dpi=300)\n",
    "    plt.show()\n",
    "    # Variables with interesting bimodal distributions: \n",
    "    # HRV_pNN50, HRV_pNN20, HRV_HTI, HRV_HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05910c3d-1cba-4d68-9843-4917a4233605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasets_conda_torch",
   "language": "python",
   "name": "datasets_conda_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
