{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57bd01-f8a0-455d-ba54-17a46718570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from bidict import bidict\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a186dd-5033-486d-a445-9f7bddfb45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "icds = pd.read_csv(\"MIMIC_III_DIAGNOSES_ICD.csv\")\n",
    "icds[icds[\"SUBJECT_ID\"] == 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd2787-e569-4b21-8d6a-c70441a9f38a",
   "metadata": {},
   "source": [
    "From mixehr training run (with_ecg_quantiles):\n",
    "* numOfPheTypes: 4\n",
    "* numOfLabTypes: 1\n",
    "* numOfPhenotypes: 27811\n",
    "* numOfLabTests: 389\n",
    "* numOfPats: 3932\n",
    "* C_train: 5061127\n",
    "* Training data file parsing completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57224d25-93a3-4830-aa5a-a397d0e45a80",
   "metadata": {},
   "source": [
    "### Summary: \n",
    "* alpha gives a single weighting to each topic\n",
    "* beta gives a single weighting to each phenotype\n",
    "* phi gives, for each phenotype (apart from labs), a weighting for each topic\n",
    "* psi does the same thing, but for labs\n",
    "* psiHyper, eta, and zeta are also specifically about labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95583fa0-1fbe-4086-860c-76d3778c1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = \"with_ecg_quantiles\"\n",
    "\n",
    "alpha = pd.read_csv(\"train_mixehr_\" + condition + \"_JCVB0_nmar_K75_iter500_alpha.csv\", header=None)\n",
    "beta = pd.read_csv(\"train_mixehr_\" + condition + \"_JCVB0_nmar_K75_iter500_beta.csv\", header=None)\n",
    "eta = pd.read_csv(\"train_mixehr_\" + condition + \"_JCVB0_nmar_K75_iter500_eta.csv\", header=None)\n",
    "eta_normalized = pd.read_csv(\"train_mixehr_\" + condition + \"_JCVB0_nmar_K75_iter500_eta_normalized.csv\", header=None)\n",
    "phi = pd.read_csv(\"train_mixehr_\" + condition + \"_JCVB0_nmar_K75_iter500_phi.csv\", header=None)\n",
    "phi_normalized = pd.read_csv(\"train_mixehr_\" + condition + \"_JCVB0_nmar_K75_iter500_phi_normalized.csv\", header=None)\n",
    "psi = pd.read_csv(\"train_mixehr_\" + condition + \"_JCVB0_nmar_K75_iter500_psi.csv\", header=None)\n",
    "psiHyper = pd.read_csv(\"train_mixehr_\" + condition + \"_JCVB0_nmar_K75_iter500_psiHyper.csv\", header=None)\n",
    "zeta = pd.read_csv(\"train_mixehr_\" + condition + \"_JCVB0_nmar_K75_iter500_zeta.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df592002-696b-4757-bfa0-47d0406d64f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade64b1-12ee-4c1e-b52c-44bc5aa1ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, MeanShift, DBSCAN\n",
    "from sklearn import decomposition\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_clusters = 12\n",
    "\n",
    "test_pt_dist = pd.read_csv(\"train_mixehr_with_ecg_quantiles_train_mixehr_with_ecg_quantiles_JCVB0_nmar_K75_iter500_metaphe.csv\", header=None)\n",
    "#test_pt_dist = pd.read_csv(\"vali_mixehr_no_waveforms_train_mixehr_no_waveforms_JCVB0_nmar_K75_iter500_metaphe.csv\", header=None)\n",
    "\n",
    "scaled_data = test_pt_dist.to_numpy()\n",
    "\n",
    "# pca = decomposition.PCA(n_components=4)\n",
    "# pca.fit(scaled_data)\n",
    "\n",
    "# pcad = pca.transform(scaled_data)\n",
    "\n",
    "# scaled_data = pcad\n",
    "\n",
    "kmeans = KMeans(init=\"random\", n_clusters=n_clusters, n_init=10, max_iter=1000, random_state=0).fit(scaled_data)\n",
    "#meanshift = MeanShift().fit(scaled_data)\n",
    "#dbscan = DBSCAN(eps=0.6, min_samples=15).fit(scaled_data)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, random_state=123)\n",
    "z = tsne.fit_transform(scaled_data)\n",
    "\n",
    "\n",
    "#df = pd.DataFrame()\n",
    "test_pt_dist[\"comp-1\"] = z[:, 1]\n",
    "test_pt_dist[\"comp-2\"] = z[:, 0]\n",
    "#df[\"comp-1\"] = scaled_data[:, 0]\n",
    "#df[\"comp-2\"] = scaled_data[:, 1]\n",
    "#df[\"comp-1\"] = pcad[:, 0]\n",
    "#df[\"comp-2\"] = pcad[:, 1]\n",
    "test_pt_dist[\"Cluster\"] = kmeans.labels_ + 1\n",
    "\n",
    "sns.set(rc={'figure.figsize':(6,5)})\n",
    "#ax = sns.scatterplot(x=\"comp-1\", y=\"comp-2\", legend=\"full\", palette=\"bright\", data=df)\n",
    "ax = sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=\"Cluster\", style=\"Cluster\", legend=\"full\", palette=\"bright\", data=test_pt_dist)\n",
    "#plt.axis(\"off\")\n",
    "#plt.legend(ncol=n_clusters)\n",
    "#ax.set_title(backgrounds[bg] + \" test set images with projection based on all feature map values\")\n",
    "#ax.legend([\"cluster \" + str(cnum) for cnum in range(1,3)])\n",
    "#ax.legend([\"cluster 1\", \"cluster 2\", \"cluster 3\"])\n",
    "\n",
    "plt.xlim([-100, 75])\n",
    "plt.ylim([-100, 75])\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"train_pt_cluster_with_ecg_quantiles.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9856259-a68a-48e0-bca6-bbfb47bf44b7",
   "metadata": {},
   "source": [
    "# Topics most associated with each cluster\n",
    "\n",
    "for c in range(n_clusters):\n",
    "    cluster_pts = test_pt_dist[test_pt_dist[\"Cluster\"] == c+1].iloc[:, :75]\n",
    "    highest_topic_weights = [round(num, 2) for num in list(cluster_pts.mean(axis=0).nlargest(n=5))]\n",
    "    highest_topics = list(cluster_pts.mean(axis=0).nlargest(n=5).index)\n",
    "    print(\"Topics most associated with cluster\", c+1, \":\", highest_topics)\n",
    "    print(highest_topic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21803614-d554-425e-a2d5-2f0e7857858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "import seaborn as sns\n",
    "\n",
    "phi = pd.read_csv(\"./train_mixehr_with_ecg_quantiles_JCVB0_nmar_K75_iter500_phi_normalized.csv\", header=None)\n",
    "\n",
    "phi = phi[phi[0] == 5] # Only ecg features\n",
    "\n",
    "#dropped_topics = [67, 27, 39, 73, 6, 12, 19, 37, 49, 63, 51, 65, 72, 74]\n",
    "dropped_topics = []\n",
    "phi = phi.drop(labels=[t+2 for t in dropped_topics], axis=1)\n",
    "\n",
    "scaled_data = phi.to_numpy()[:, 2:].T\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "pcad = pca.transform(scaled_data)\n",
    "\n",
    "scaled_data = pcad\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"comp1\"] = pcad[:, 0]\n",
    "df[\"comp2\"] = pcad[:, 1]\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "p1 = sns.scatterplot(x=\"comp1\", # Horizontal axis\n",
    "       y=\"comp2\", # Vertical axis\n",
    "       data=df, # Data source\n",
    "       size = 8,\n",
    "       legend=False)  \n",
    "\n",
    "#labelled_points = [12, 63]\n",
    "labels = list(range(75))\n",
    "for t in dropped_topics:\n",
    "    labels.remove(t)\n",
    "df[\"label\"] = labels\n",
    "#df[\"label\"] = df.apply(lambda row: str(int(row[\"label\"])) if row[\"label\"] in labelled_points else \"\", axis=1)\n",
    "\n",
    "#df = df[(df[\"comp1\"] > -0.05) & (df[\"comp1\"] < 0.05) & (df[\"comp2\"] > -0.05) & (df[\"comp2\"] < 0.05)]\n",
    "\n",
    "for line in range(0,df.shape[0]):\n",
    "    if line in df[\"label\"]:\n",
    "        p1.text(df.comp1[line]+0.001, df.comp2[line], \n",
    "        df.label[line], horizontalalignment='left',\n",
    "        size='small', fontsize=16, color='black', weight='semibold')\n",
    "\n",
    "\n",
    "# plt.xlim([-0.02, 0.02])\n",
    "# plt.ylim([-0.02, 0.02])\n",
    "        \n",
    "#plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ecg_pca_norm.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d0bac-96d9-4e2a-b299-771e0e5ec0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "import seaborn as sns\n",
    "\n",
    "phi = pd.read_csv(\"./train_mixehr_with_ecg_quantiles_JCVB0_nmar_K75_iter500_phi.csv\", header=None)\n",
    "\n",
    "phi = phi[phi[0] == 5] # Only ecg features\n",
    "\n",
    "scaled_data = phi.to_numpy()[:, 2:].T\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "pcad = pca.transform(scaled_data)\n",
    "\n",
    "scaled_data = pcad\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"comp1\"] = pcad[:, 0]\n",
    "df[\"comp2\"] = pcad[:, 1]\n",
    "\n",
    "plt.figure(figsize=(4,2))\n",
    "p1 = sns.scatterplot(x=\"comp1\", # Horizontal axis\n",
    "       y=\"comp2\", # Vertical axis\n",
    "       data=df, # Data source\n",
    "       size = 8,\n",
    "       legend=False)  \n",
    "\n",
    "labelled_points = [32, 34, 50, 52, 66, 68, 72, 74]\n",
    "df[\"label\"] = list(range(75))\n",
    "df[\"label\"] = df.apply(lambda row: str(int(row[\"label\"])) if row[\"label\"] in labelled_points else \"\", axis=1)\n",
    "\n",
    "for line in range(0,df.shape[0]):\n",
    "     p1.text(df.comp1[line]+0.01, df.comp2[line], \n",
    "     df.label[line], horizontalalignment='left',\n",
    "     size='small', fontsize=10, color='black', weight='semibold')\n",
    "        \n",
    "        \n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ecg_pca.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18dfe3-ac79-4133-acc2-8bf1fa8fdc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "import seaborn as sns\n",
    "\n",
    "phi = pd.read_csv(\"./train_mixehr_with_ecg_quantiles_JCVB0_nmar_K75_iter500_phi.csv\", header=None)\n",
    "\n",
    "phi = phi[phi[0] == 5] # Only ecg features\n",
    "\n",
    "scaled_data = phi.to_numpy()[:, 2:].T\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "pcad = pca.transform(scaled_data)\n",
    "\n",
    "scaled_data = pcad\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"comp1\"] = pcad[:, 0]\n",
    "df[\"comp2\"] = pcad[:, 1]\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "p1 = sns.scatterplot(x=\"comp1\", # Horizontal axis\n",
    "       y=\"comp2\", # Vertical axis\n",
    "       data=df, # Data source\n",
    "       size = 8,\n",
    "       legend=False)  \n",
    "\n",
    "#labelled_points = [19, 32, 34, 50, 52, 66, 68, 72, 74]\n",
    "df[\"label\"] = list(range(75))\n",
    "#df[\"label\"] = df.apply(lambda row: str(int(row[\"label\"])) if row[\"label\"] in labelled_points else \"\", axis=1)\n",
    "\n",
    "df = df[(df[\"comp1\"] > -1000) & (df[\"comp1\"] < 1000) & (df[\"comp2\"] > -1000) & (df[\"comp2\"] < 1000)]\n",
    "\n",
    "for line in range(0,df.shape[0]):\n",
    "    if line in df[\"label\"]:\n",
    "        p1.text(df.comp1[line]+0.01, df.comp2[line], \n",
    "        df.label[line], horizontalalignment='left',\n",
    "        size='small', fontsize=16, color='black', weight='semibold')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim([-1000, 1010])\n",
    "plt.ylim([-1000, 1000])\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ecg_pca.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc94b22-cc92-4c44-ac0c-ff03a716ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "import seaborn as sns\n",
    "\n",
    "phi = pd.read_csv(\"./train_mixehr_only_ecg_quantiles_JCVB0_nmar_K75_iter500_phi.csv\", header=None)\n",
    "\n",
    "phi = phi[phi[0] == 5] # Only ecg features\n",
    "\n",
    "scaled_data = phi.to_numpy()[:, 2:].T\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "pcad = pca.transform(scaled_data)\n",
    "\n",
    "scaled_data = pcad\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"comp1\"] = pcad[:, 0]\n",
    "df[\"comp2\"] = pcad[:, 1]\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "p1 = sns.scatterplot(x=\"comp1\", # Horizontal axis\n",
    "       y=\"comp2\", # Vertical axis\n",
    "       data=df, # Data source\n",
    "       size = 8,\n",
    "       legend=False)  \n",
    "\n",
    "#labelled_points = [19, 32, 34, 50, 52, 66, 68, 72, 74]\n",
    "df[\"label\"] = list(range(75))\n",
    "#df[\"label\"] = df.apply(lambda row: str(int(row[\"label\"])) if row[\"label\"] in labelled_points else \"\", axis=1)\n",
    "\n",
    "for line in range(0,df.shape[0]):\n",
    "     p1.text(df.comp1[line]+0.01, df.comp2[line], \n",
    "     df.label[line], horizontalalignment='left',\n",
    "     size='small', fontsize=16, color='black', weight='semibold')\n",
    "        \n",
    "        \n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ecg_pca.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e52da-da01-4bd4-8f30-2503f189d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pt_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f77028-5dad-4dc6-a396-fe58830a8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, MeanShift, DBSCAN\n",
    "from sklearn import decomposition\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_clusters = 9\n",
    "\n",
    "val_pt_dist = pd.read_csv(\"vali_mixehr_with_ecg_quantiles_train_mixehr_with_ecg_quantiles_JCVB0_nmar_K75_iter500_metaphe.csv\", header=None)\n",
    "\n",
    "scaled_data = val_pt_dist.to_numpy()\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "pcad = pca.transform(scaled_data)\n",
    "\n",
    "scaled_data = pcad\n",
    "\n",
    "#kmeans = KMeans(init=\"random\", n_clusters=n_clusters, n_init=10, max_iter=1000, random_state=0).fit(scaled_data)\n",
    "meanshift = MeanShift().fit(scaled_data)\n",
    "#dbscan = DBSCAN(eps=0.6, min_samples=15).fit(scaled_data)\n",
    "\n",
    "# tsne = TSNE(n_components=2, verbose=1, random_state=123)\n",
    "# z = tsne.fit_transform(scaled_data)\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"comp-1\"] = pcad[:, 0]\n",
    "df[\"comp-2\"] = pcad[:, 1]\n",
    "df[\"Cluster\"] = meanshift.labels_ + 1\n",
    "\n",
    "sns.set(rc={'figure.figsize':(6,5)})\n",
    "#ax = sns.scatterplot(x=\"comp-1\", y=\"comp-2\", legend=\"full\", palette=\"bright\", data=df)\n",
    "ax = sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=\"Cluster\", style=\"Cluster\", legend=\"full\", palette=\"bright\", data=df)\n",
    "plt.axis(\"off\")\n",
    "#plt.legend(ncol=n_clusters)\n",
    "#ax.set_title(backgrounds[bg] + \" test set images with projection based on all feature map values\")\n",
    "#ax.legend([\"cluster \" + str(cnum) for cnum in range(1,3)])\n",
    "#ax.legend([\"cluster 1\", \"cluster 2\", \"cluster 3\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"vali_pca_cluster_with_ecg_quantiles.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55250e5c-091b-42c3-9692-9121e3bcb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d2b9b-dd8b-4975-a554-654f8819f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beta[0].unique()) # Missing the \"lab\" typeId\n",
    "beta # Does this show the fraction of patients who had this phenotype? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc32fb-77cf-4c45-84c3-3e6520f14728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f4a39-1520-463a-8a0c-23e5688b5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta # 778, seems to be related to specifically labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec40aa1-a40f-4c13-8187-83e4928cf78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_normalized # Normalized how?? All the values are between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3ae93-0c0a-4776-9e34-ce611209aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phi[0].unique()) # Also missing lab typeIds\n",
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab88efa-4603-4a62-bb0f-6a3953e18453",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi # Seems to be more labs, but without regard to state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6f038-d431-4fa3-8f23-6bf44e764b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "psiHyper # More labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec237879-45c8-4901-8515-19c381829b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta # Also labs, with states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed8792-9278-4208-9023-89e577b0e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv(\"train_mixehr_with_ecg_quantiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49f2f0-7c84-4a24-9f1b-b83d09c9d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata[\"ALL_PHE\"] = traindata[\"typeId\"].astype(str) + \"-\" + traindata[\"pheId\"].astype(str)\n",
    "traindata[\"ALL_PHE\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db4c31-4cb5-4457-9455-1f86e7df4f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasets_conda_torch",
   "language": "python",
   "name": "datasets_conda_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
